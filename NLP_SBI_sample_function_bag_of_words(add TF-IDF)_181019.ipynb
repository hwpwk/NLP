{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tomori kengo\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import neologdn\n",
    "import MeCab\n",
    "\n",
    "import re\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# bag of wordsを作成するためのライブラリ\n",
    "from gensim import corpora, matutils\n",
    "# TF-IDFを作成するためのライブラリ\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding='utf-8'で上手くいった\n",
    "diary_df = pd.read_csv('SBI_Financial statement_201903.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>国内経済が緩やかに回復するなか、マーケット環境は、米国と中国の貿易摩擦問題に対する警戒感等か...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>このような環境の中、当社業績においては、ホールセールビジネスの拡大、トレーディング収益や金融...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>当社は引続き他社を大きく上回る高いシェアを維持し、35.9％のシェアを獲得。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>先物・オプションの委託個人売買代金シェアは、引き続き高水準を維持。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>投資信託残高の四半期末残高は過去最高を更新し、信託報酬は高水準を維持。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018年4月から2018年6月までの上場会社数は20社。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>同期間のSBI証券引受関与率は100％と 引き続き業界トップ。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  国内経済が緩やかに回復するなか、マーケット環境は、米国と中国の貿易摩擦問題に対する警戒感等か...\n",
       "1  このような環境の中、当社業績においては、ホールセールビジネスの拡大、トレーディング収益や金融...\n",
       "2             当社は引続き他社を大きく上回る高いシェアを維持し、35.9％のシェアを獲得。\n",
       "3                  先物・オプションの委託個人売買代金シェアは、引き続き高水準を維持。\n",
       "4                投資信託残高の四半期末残高は過去最高を更新し、信託報酬は高水準を維持。\n",
       "5                      2018年4月から2018年6月までの上場会社数は20社。\n",
       "6                    同期間のSBI証券引受関与率は100％と 引き続き業界トップ。"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeCabにかける前準備としてneologdn.normalize()を使用して文章全体を正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_diary_normalization(text):\n",
    "    diary_normalization = neologdn.normalize(text)\n",
    "    return diary_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeCab + neologdで形態素解析し、名詞、形容詞原形と動詞原形を抽出しリストに格納\n",
    "##### 参考：https://github.com/kujirahand/book-mlearn-gyomu/blob/master/src/ch4/Doc2Vec/create_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neologd_tagger = MeCab.Tagger('-Ochasen -d C:\\mecab-ipadic-neologd')\n",
    "\n",
    "# 引数のテキストを分かち書きして配列にする\n",
    "# node.surface: 文字のみ取得できる 出力例：同  期間  の  SBI  証券\n",
    "# node.feature: 品詞、原形などの詳細を取得できる:「品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音」の順\n",
    "# よって、「品詞」を取得したい場合はnode.featureをsplit()でリスト型にした後に抽出したい詳細のインデックス番号[0]を指定すればよい\n",
    "# また、動詞や形容詞の「原形（の単語）」を取得したい場合はインデックス番号[6]を指定すればよい\n",
    "\n",
    "def split_words(diary_normalization):\n",
    "    node = neologd_tagger.parseToNode(diary_normalization) #parseだとエラー「 'str' object has no attribute 'feature'」\n",
    "    wakati_words = []\n",
    "    while node is not None:\n",
    "        hinshi = node.feature.split(\",\")[0]\n",
    "        if  hinshi in [\"名詞\"]:\n",
    "            wakati_words.append(node.surface)\n",
    "        elif hinshi in [\"動詞\", \"形容詞\"]:\n",
    "            wakati_words.append(node.feature.split(\",\")[6])\n",
    "        node = node.next\n",
    "    return wakati_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['国内',\n",
       "  '経済',\n",
       "  '緩やか',\n",
       "  '回復',\n",
       "  'する',\n",
       "  'なか',\n",
       "  'マーケット',\n",
       "  '環境',\n",
       "  '米国',\n",
       "  '中国',\n",
       "  '貿易摩擦',\n",
       "  '問題',\n",
       "  '警戒感',\n",
       "  '等',\n",
       "  '一進一退',\n",
       "  '展開'],\n",
       " ['よう',\n",
       "  '環境',\n",
       "  '中',\n",
       "  '当社',\n",
       "  '業績',\n",
       "  'ホールセール',\n",
       "  'ビジネス',\n",
       "  '拡大',\n",
       "  'トレーディング',\n",
       "  '収益',\n",
       "  '金融',\n",
       "  '収益',\n",
       "  '増加',\n",
       "  '前年同期',\n",
       "  '比',\n",
       "  '増収増益',\n",
       "  '達成'],\n",
       " ['当社', '他社', '大きい', '上回る', '高い', 'シェア', '維持', 'する', '35.9%', 'シェア', '獲得'],\n",
       " ['先物', 'オプション', '委託', '個人', '売買代金', 'シェア', '高水準', '維持'],\n",
       " ['投資信託', '残高', '四半期', '末', '残高', '過去最高', '更新', 'する', '信託報酬', '高水準', '維持'],\n",
       " ['2018年', '4月', '2018年', '6月', '上場会社', '数', '20', '社'],\n",
       " ['期間', 'SBI証券', '引受', '関与', '率', '100%', '業界', 'トップ']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wakati_list = []\n",
    "\n",
    "# 文章の上から順に作成した関数を実行\n",
    "for diary in diary_df['text']:\n",
    "    diary_normalization = get_diary_normalization(diary)\n",
    "    wakati_words = split_words(diary_normalization)\n",
    "    wakati_list.append(wakati_words)\n",
    "    \n",
    "wakati_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of Wordsの実装\n",
    "##### 参考書籍：前処理大全[データ分析のためのSQL/R/Python実践テクニック] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bag of words作成準備\n",
    "# 全種類の単語のIDを付与した辞書の作成\n",
    "corpus_dic = corpora.Dictionary(wakati_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 各文章の単語リストをコーパス（辞書の単語IDと単語の出現回数）リストに変換\n",
    "# 辞書オブジェクトからdoc2bow関数を呼び出し、引数に単語リストを指定することで辞書に基づいたコーパスを取得できる\n",
    "# コーパスとは、文章に含まれる単語を、辞書の単語IDと単語の出現回数の組み合わせに変換したもの\n",
    "# bag of wordsに変換するときは変換元のデータ番号が行列の行番号に、単語IDの番号が行列の列番号に、出現回数が行列の値に該当\n",
    "corpus_list = [corpus_dic.doc2bow(wakati) for wakati in wakati_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matutilsオブジェクトのcorpus2csc関数：引数に指定したコーパスリストをスパースマトリックス(CSC型)のbag of wordsに変換する\n",
    "wakati_matrix = matutils.corpus2csc(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 0)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 0)\t1.0\n",
      "  (12, 0)\t1.0\n",
      "  (13, 0)\t1.0\n",
      "  (14, 0)\t1.0\n",
      "  (15, 0)\t1.0\n",
      "  (9, 1)\t1.0\n",
      "  (16, 1)\t1.0\n",
      "  (17, 1)\t1.0\n",
      "  (18, 1)\t1.0\n",
      "  (19, 1)\t1.0\n",
      "  (20, 1)\t1.0\n",
      "  (21, 1)\t1.0\n",
      "  (22, 1)\t2.0\n",
      "  (23, 1)\t1.0\n",
      "  :\t:\n",
      "  (0, 4)\t1.0\n",
      "  (37, 4)\t1.0\n",
      "  (44, 4)\t1.0\n",
      "  (45, 4)\t1.0\n",
      "  (46, 4)\t1.0\n",
      "  (47, 4)\t1.0\n",
      "  (48, 4)\t1.0\n",
      "  (49, 4)\t1.0\n",
      "  (50, 4)\t2.0\n",
      "  (51, 4)\t1.0\n",
      "  (52, 5)\t1.0\n",
      "  (53, 5)\t2.0\n",
      "  (54, 5)\t1.0\n",
      "  (55, 5)\t1.0\n",
      "  (56, 5)\t1.0\n",
      "  (57, 5)\t1.0\n",
      "  (58, 5)\t1.0\n",
      "  (59, 6)\t1.0\n",
      "  (60, 6)\t1.0\n",
      "  (61, 6)\t1.0\n",
      "  (62, 6)\t1.0\n",
      "  (63, 6)\t1.0\n",
      "  (64, 6)\t1.0\n",
      "  (65, 6)\t1.0\n",
      "  (66, 6)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(wakati_matrix)# printなしの「wakati_matrix」だけだと<67x7 sparse matrix of type '<class 'numpy.float64'>'と出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 2. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 2. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 2. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(wakati_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDFを利用したbag of wordsの作成\n",
    "##### 参考書籍：前処理大全[データ分析のためのSQL/R/Python実践テクニック]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-IDFモデルの作成\n",
    "# models.TfidfModel関数はコーパスリストをTF-IDF値に変換するオブジェクトを生成する関数\n",
    "# 引数にコーパスリストを指定し、normalize引数をTrueにするとTF-IDF値をL2ノルムによって正規化した値に変換するオブジェクトになる\n",
    "tfidf_model = models.TfidfModel(corpus_list, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpusにTF-IDFを適用\n",
    "corpus_list_tfidf = tfidf_model[corpus_list]\n",
    "wakati_tfidf_matrix = matutils.corpus2csc(corpus_list_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11394006 0.         0.14977713 0.         0.13253451 0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.16846483 0.15246964 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.26167554 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.2368303  0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.2368303  0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.2368303  0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.2368303  0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.2368303  0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.2368303  0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.47366059 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.2368303  0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.2368303  0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.15246964 0.22145134 0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.2368303  0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.2368303  0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.2368303  0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.2368303  0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.2368303  0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.3439792  0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.44290268 0.26242233 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.3439792  0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.3439792  0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.3439792  0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.3439792  0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.14977713 0.17748759 0.13253451 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.3439792  0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.40761923 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.40761923 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.40761923 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.40761923 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.40761923 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.26242233 0.19595745 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.30437968 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.30437968 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.30437968 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.30437968 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.30437968 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.60875936 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.30437968 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.31622777\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.63245553\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.31622777\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.31622777\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.31622777\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.31622777\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.31622777\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.35355339]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.35355339]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.35355339]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.35355339]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.35355339]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.35355339]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.35355339]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.35355339]]\n"
     ]
    }
   ],
   "source": [
    "print(wakati_tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Wordsの実装2\n",
    "##### 参考URL：https://www.pytry3g.com/entry/2018/03/21/181514#Bag-of-Words%E3%81%A3%E3%81%A6%E4%BD%95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100%': 64,\n",
       " '20': 57,\n",
       " '2018年': 52,\n",
       " '35.9%': 37,\n",
       " '4月': 53,\n",
       " '6月': 54,\n",
       " 'SBI証券': 60,\n",
       " 'する': 4,\n",
       " 'なか': 5,\n",
       " 'よう': 16,\n",
       " 'オプション': 40,\n",
       " 'シェア': 35,\n",
       " 'トップ': 66,\n",
       " 'トレーディング': 23,\n",
       " 'ビジネス': 21,\n",
       " 'ホールセール': 20,\n",
       " 'マーケット': 6,\n",
       " '一進一退': 14,\n",
       " '上回る': 33,\n",
       " '上場会社': 55,\n",
       " '中': 17,\n",
       " '中国': 9,\n",
       " '他社': 31,\n",
       " '信託報酬': 51,\n",
       " '個人': 42,\n",
       " '先物': 39,\n",
       " '前年同期': 27,\n",
       " '収益': 24,\n",
       " '問題': 11,\n",
       " '四半期': 47,\n",
       " '回復': 3,\n",
       " '国内': 0,\n",
       " '増加': 26,\n",
       " '増収増益': 29,\n",
       " '売買代金': 43,\n",
       " '大きい': 32,\n",
       " '委託': 41,\n",
       " '展開': 15,\n",
       " '引受': 61,\n",
       " '当社': 18,\n",
       " '投資信託': 45,\n",
       " '拡大': 22,\n",
       " '数': 56,\n",
       " '更新': 50,\n",
       " '期間': 59,\n",
       " '末': 48,\n",
       " '業界': 65,\n",
       " '業績': 19,\n",
       " '残高': 46,\n",
       " '比': 28,\n",
       " '獲得': 38,\n",
       " '率': 63,\n",
       " '環境': 7,\n",
       " '社': 58,\n",
       " '等': 13,\n",
       " '米国': 8,\n",
       " '経済': 1,\n",
       " '維持': 36,\n",
       " '緩やか': 2,\n",
       " '警戒感': 12,\n",
       " '貿易摩擦': 10,\n",
       " '過去最高': 49,\n",
       " '達成': 30,\n",
       " '金融': 25,\n",
       " '関与': 62,\n",
       " '高い': 34,\n",
       " '高水準': 44}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 形態素解析した単語にidを割り振る\n",
    "wakati_dic = {}# {単語:id}\n",
    "for wakati in wakati_list:# wakatiには['先物', 'オプション', '委託', '個人', '売買代金', 'シェア', '高水準', '維持']と1文章すべてに含まれる単語のリストが入っている\n",
    "    for word in wakati:# wordには「国内」、「経済」、、、など各単語が入っている\n",
    "        if word in wakati_dic:#もしwordがwakati_dicの中に含まれていれば単語にidを割り振る処理をスキップする(単語の重複を除く)\n",
    "            continue\n",
    "        wakati_dic[word] = len(wakati_dic)# key(今回はword)にvalue(今回はid)を割り振る 辞書型の要素数取得にもlen()を使用できる\n",
    "wakati_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'する': 0, 'なか': 1, 'マーケット': 2, '一進一退': 3, '中国': 4, '問題': 5, '回復': 6, '国内': 7, '展開': 8, '環境': 9, '等': 10, '米国': 11, '経済': 12, '緩やか': 13, '警戒感': 14, '貿易摩擦': 15, 'よう': 16, 'トレーディング': 17, 'ビジネス': 18, 'ホールセール': 19, '中': 20, '前年同期': 21, '収益': 22, '増加': 23, '増収増益': 24, '当社': 25, '拡大': 26, '業績': 27, '比': 28, '達成': 29, '金融': 30, '35.9%': 31, 'シェア': 32, '上回る': 33, '他社': 34, '大きい': 35, '獲得': 36, '維持': 37, '高い': 38, 'オプション': 39, '個人': 40, '先物': 41, '売買代金': 42, '委託': 43, '高水準': 44, '信託報酬': 45, '四半期': 46, '投資信託': 47, '更新': 48, '末': 49, '残高': 50, '過去最高': 51, '20': 52, '2018年': 53, '4月': 54, '6月': 55, '上場会社': 56, '数': 57, '社': 58, '100%': 59, 'SBI証券': 60, 'トップ': 61, '引受': 62, '期間': 63, '業界': 64, '率': 65, '関与': 66}\n"
     ]
    }
   ],
   "source": [
    "# gensimで特徴語辞書を作成\n",
    "# 参考URL：https://qiita.com/yasunori/items/31a23eb259482e4824e2\n",
    "from gensim import corpora\n",
    "\n",
    "# words はさっきの単語リスト\n",
    "dictionary = corpora.Dictionary(wakati_list)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]\n",
      "[(9, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1)]\n",
      "[(0, 1), (25, 1), (31, 1), (32, 2), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1)]\n",
      "[(32, 1), (37, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1)]\n",
      "[(0, 1), (37, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 2), (51, 1)]\n",
      "[(52, 1), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1)]\n",
      "[(59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1)]\n"
     ]
    }
   ],
   "source": [
    "# BoWの要領で各文章の特徴語をカウントして特徴ベクトル作る\n",
    "# https://qiita.com/yasunori/items/31a23eb259482e4824e2\n",
    "for word in wakati_list:\n",
    "    vec = dictionary.doc2bow(word)\n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
