{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import neologdn\n",
    "import MeCab\n",
    "\n",
    "import re\n",
    "\n",
    "# 可視化用のライブラリ\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストデータの読み込み\n",
    "読み込むデータは：https://www.aozora.gr.jp/cards/001779/card56680.html　のテキストファイル(ルビあり)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('sakasuno_kaijin.txt', 'r') as f:とすると「'cp932' codec can't decode byte 0xef in position 0: illegal multibyte sequence」のエラーが発生\n",
    "# with open('sakasuno_kaijin.txt', 'rb') as f:\n",
    "    # text = f.read()\n",
    "# これだと永遠と「b'\\xef\\xbb\\xbf\\xe3\\x82.....」が読み込まれるのでdecodeする必要あり\n",
    "with open('sakasuno_kaijin.txt', 'rb') as f:\n",
    "    binary_data = f.read()\n",
    "# メモ帳の文字コードを「UTF-8」で保存していた場合はdecode('utf-8')\n",
    "# text = binary_data.decode('utf-8')\n",
    "# メモ帳の文字コードを「ANSI」で保存していた場合はdecode('shift-jis')\n",
    "text = binary_data.decode('shift-jis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正規表現で該当文字列を削除\n",
    "#### タイトル名、作者名、-（ハイフン）内の文字列を削除\n",
    "##### 参考URL：http://www-creators.com/archives/4278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -（ハイフン）が5つ以上:-{5,}\n",
    "# {n,}:直前の文字がn回以上に一致\n",
    "# 参考URL：http://hodade.com/seiki/page.php?study1\n",
    "# split関数を使うことでtextはリスト型になる\n",
    "# 今回は-（ハイフン）が5つ以上の箇所で区切ることになるのでtext[0]=タイトル名、作者名 text[1]=【テキスト中に現れる記号について】text[2]=本文 となる\n",
    "# -（ハイフン）の前に「\\」を付けることで正規表現の-（ハイフン）ではなく単なる文字としての-（ハイフン）という意味に変更している\n",
    "# 「\\」:直後の文字を正規表現の記号（メタ文字）として扱わないことを指定\n",
    "text = re.split('\\-{5,}', text)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文末の「（スペース）底本：」以降の文字列を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = re.split('底本：', text)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 空白文字(スペースやタブ、改行)を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# strip関数は文字の間に入っている空白を削除しない\n",
    "text = text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 《》：ルビを削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 置換後の文字列 = re.sub(正規表現, 置換後文字, 置換される文字列 [, 置換回数])\n",
    "# 「.」:あらゆる一文字 「.」を４つ並べば、何の文字かを問わずに「４文字の文字列」を表す\n",
    "# しかしながら、文字数の分だけ何度も「.」を繰り返し入力するのは大変。\n",
    "# そんな時、そのパターンが何回繰り返されるかを単純に表現するのが「+」\n",
    "# 「+」:直前のパターンの１回以上の繰り返しを表す\n",
    "# 「?」:０回か、１回の繰り返し\n",
    "text = re.sub('《.+》', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ｜：ルビの付く文字列の始まりを特定する記号を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text.replace('｜', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ［＃］：入力者注　主に外字の説明や、傍点の位置の指定 する記号を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = re.sub('［＃.+］', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neologdnで正規化、Mecab + neologd 辞書による形態素解析、品詞で単語を絞った後に出現頻度10以上の単語を抽出してリストに格納する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mecabed_word_list(text):\n",
    "    text_normalization = neologdn.normalize(text)\n",
    "    neologd_tagger = MeCab.Tagger('-Ochasen -d C:\\mecab-ipadic-neologd')\n",
    "    \n",
    "    # neologd_tagger.parse(text)で各単語の原形、品詞などが1行で連続して表示される\n",
    "    # 原形、品詞などの間には「\\t」が、分かち書きされた単語と単語の区切りには「\\n」が表示される\n",
    "    # 例: '空い\\tアイ\\t空く\\t動詞-自立\\t五段・カ行イ音便\\t連用タ接続\\nた\\tタ\\tた\\t助動詞\\t特殊・タ\\t基本形\\n時間....\n",
    "    # まずはparse()で分かち書きした単語群は1つの文字列型になっているので「\\n」で区切り、リスト型にする\n",
    "    wakati_text_list = neologd_tagger.parse(text_normalization).split('\\n')\n",
    "    # 「\\n」で区切り、リスト型にした結果の例は下記\n",
    "    # ['空い\\tアイ\\t空く\\t動詞-自立\\t五段・カ行イ音便\\t連用タ接続', ：リスト0番目\n",
    "    # 'た\\tタ\\tた\\t助動詞\\t特殊・タ\\t基本形',：リスト1番目\n",
    "    # '時間\\tジカン\\t時間\\t名詞-副詞可能\\t\\t',：リスト2番目\n",
    "    \n",
    "    ##【形態素解析結果を格納したリストから特定の品詞（品詞詳細部分まで考慮に入れた場合）のみ抽出】\n",
    "    # 抽出したい品詞のリストを作成（完全一致）\n",
    "    # 品詞参考URL：http://miner.hatenablog.com/entry/323\n",
    "    hinshi_list = ['名詞-一般', '名詞-形容動詞語幹', '名詞-固有名詞-一般',  '名詞-サ変接続', '形容詞-自立', '形容詞-接尾', '形容詞-非自立', '動詞-自立', '動詞-接尾', '動詞-非自立', '副詞-一般', '副詞-助詞類接続']\n",
    "    # hinshi_list = ('名詞-一般', '名詞-サ変接続', '名詞-固有名詞', '名詞-形容動詞語幹'...)とタプルでも同じ結果\n",
    "    original_form_list =[]# 単語の「原形」のみ格納する(品詞情報は単語頻度を求める際に必要ないので除外)\n",
    "    \n",
    "    # parse() の出力結果の最後は「EOS」という文字のみ\n",
    "    # EOSのとき、pos = wakati.split('\\t')[3]の要素はないので下記forループを実行すると「list index out of range」とエラーを発生させてしまう\n",
    "    # よってEOSのときは条件分岐if~breakでforループから抜け出すよう記述\n",
    "    for wakati in wakati_text_list:\n",
    "        surface = wakati.split('\\t')[0]\n",
    "        if surface == 'EOS':\n",
    "            break\n",
    "        else:\n",
    "            pos = wakati.split('\\t')[3]\n",
    "            if pos in hinshi_list:# posはhinshi_listの中の要素と完全一致していないと抽出できない\n",
    "                original_form_list.append(wakati.split('\\t')[2])\n",
    "    # ここまでで品詞によって単語を絞った\n",
    "    \n",
    "    ##【単語の出現頻度を求め、出現頻度10以上の単語のみ抽出】\n",
    "    # まずは単語の出現頻度を求める\n",
    "    import collections\n",
    "    count = collections.Counter(original_form_list)\n",
    "    word_count_list = count.most_common()\n",
    "    word_count_list# word_count_listは全体がリスト型でリスト内に('する', 4), ('ぼんやり', 3),,,,といった単語とその出現頻度がタプルで格納されている\n",
    "    # 次に出現頻度が10以上の単語のみ抽出する\n",
    "    Frequency10_word_list = []\n",
    "    for word_count in word_count_list:\n",
    "        if word_count[1] > 9:\n",
    "            Frequency10_word_list.append(word_count[0])\n",
    "    \n",
    "    return Frequency10_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽出された単語数は304語です。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['いる',\n",
       " 'する',\n",
       " '骸骨',\n",
       " 'ある',\n",
       " '男',\n",
       " 'くる',\n",
       " 'なる',\n",
       " 'いく',\n",
       " '見る',\n",
       " 'しまう',\n",
       " 'いう',\n",
       " '顔',\n",
       " 'サーカス',\n",
       " 'れる',\n",
       " 'ない',\n",
       " '明智',\n",
       " '警官',\n",
       " 'そう',\n",
       " 'テント',\n",
       " 'バス',\n",
       " 'できる',\n",
       " '少年',\n",
       " 'もう',\n",
       " 'わかる',\n",
       " '見える',\n",
       " 'ひとり',\n",
       " '思う',\n",
       " 'られる',\n",
       " 'ふたり',\n",
       " '下',\n",
       " '出る',\n",
       " 'うしろ',\n",
       " '目',\n",
       " 'はいる',\n",
       " '声',\n",
       " 'ドア',\n",
       " '恐ろしい',\n",
       " '無効',\n",
       " '人',\n",
       " 'みる',\n",
       " '逃げる',\n",
       " '女中',\n",
       " '見物',\n",
       " '自動車',\n",
       " 'あと',\n",
       " '面相',\n",
       " 'せる',\n",
       " '鉄',\n",
       " 'つく',\n",
       " 'ゾウ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 関数を実行\n",
    "mecabed_word_list = get_mecabed_word_list(text)\n",
    "print('抽出された単語数は{}語です。'.format(len(mecabed_word_list)))\n",
    "mecabed_word_list[:50]# インデックス番号0~49までの出現頻度上位50単語のみ確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 参考1:udemy講座「【TensorFlow・Keras・Python3で学ぶ】時系列データ処理入門（RNN/LSTM, Word2Vec)」\n",
    "##### 参考2:https://datumstudio.jp/blog/python%e3%81%ab%e3%82%88%e3%82%8b%e6%97%a5%e6%9c%ac%e8%aa%9e%e5%89%8d%e5%87%a6%e7%90%86%e5%82%99%e5%bf%98%e9%8c%b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
