{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tomori kengo\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import neologdn\n",
    "import MeCab\n",
    "\n",
    "import re\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora# 辞書作成用\n",
    "from gensim import models# LDA用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日誌サンプルデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>posi_nega</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>案件個別の技術相談乗ってます！乗ります</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>勉強とお仕事が有機的に繋がっていて面白い。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>次々とテクニックを教えていただき、ためになりました</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>お肉がキレイに焼けたことに小さなしあわせを感じた</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>これが私の最適化</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        text  posi_nega\n",
       "0        案件個別の技術相談乗ってます！乗ります          1\n",
       "1      勉強とお仕事が有機的に繋がっていて面白い。          1\n",
       "2  次々とテクニックを教えていただき、ためになりました          1\n",
       "3   お肉がキレイに焼けたことに小さなしあわせを感じた          1\n",
       "4                   これが私の最適化          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoding='utf-8'で上手くいった\n",
    "df = pd.read_csv('posi_nega_alldata.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeCabにかける前準備としてneologdn.normalize()を使用して文章全体を正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_diary_normalization(text):\n",
    "    diary_normalization = neologdn.normalize(text)\n",
    "    return diary_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeCab + neologdで形態素解析し、名詞、形容詞原形と動詞原形を抽出しリストに格納\n",
    "##### 参考：https://github.com/kujirahand/book-mlearn-gyomu/blob/master/src/ch4/Doc2Vec/create_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neologd_tagger = MeCab.Tagger('-Ochasen -d C:\\mecab-ipadic-neologd')\n",
    "\n",
    "# 引数のテキストを分かち書きして配列にする\n",
    "# node.surface: 文字のみ取得できる 出力例：同  期間  の  SBI  証券\n",
    "# node.feature: 品詞、原形などの詳細を取得できる:「品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音」の順\n",
    "# よって、「品詞」を取得したい場合はnode.featureをsplit()でリスト型にした後に抽出したい詳細のインデックス番号[0]を指定すればよい\n",
    "# また、動詞や形容詞の「原形（の単語）」を取得したい場合はインデックス番号[6]を指定すればよい\n",
    "\n",
    "def split_words(diary_normalization):\n",
    "    node = neologd_tagger.parseToNode(diary_normalization) #parseだとエラー「 'str' object has no attribute 'feature'」\n",
    "    wakati_words = []\n",
    "    while node is not None:\n",
    "        hinshi = node.feature.split(\",\")[0]\n",
    "        if  hinshi in [\"名詞\"]:\n",
    "            wakati_words.append(node.surface)\n",
    "        elif hinshi in [\"形容詞\"]:\n",
    "            wakati_words.append(node.feature.split(\",\")[6])\n",
    "        node = node.next\n",
    "    return wakati_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['案件', '個別', '技術', '相談'],\n",
       " ['勉強', 'お仕事', '有機的', '面白い'],\n",
       " ['テクニック', 'ため'],\n",
       " ['お肉', 'キレイ', 'こと', 'しあわせ'],\n",
       " ['これ', '私', '最適化'],\n",
       " ['データ', '抽出', 'グラフ', '化', '問題', 'さ', 'そう'],\n",
       " ['PC', '復帰', 'みんな', '日報', 'コメント'],\n",
       " ['久々', '新卒', '自社', '嬉しい'],\n",
       " ['おでん', '美味しい', '季節'],\n",
       " ['昼飯', '割高', '値段', '満足感'],\n",
       " ['10歳', '以上', '若い', '判定'],\n",
       " ['2週', '連続', '出展'],\n",
       " ['参加', '皆様', '皆様'],\n",
       " ['無事', 'こと'],\n",
       " ['これ', '協力', '皆様', 'おかげ'],\n",
       " ['本当にありがとうございました'],\n",
       " ['本', 'モデル', '会社', 'サービス', 'ビジネスモデル', '歴史', '軽い', '紹介', 'おもしろい'],\n",
       " ['ベイズ', '統計', 'モデリング', 'こと'],\n",
       " ['休日', 'オフィス', '出社', '静か', 'さ', '良い'],\n",
       " ['すごい', 'いい', '話', '気'],\n",
       " ['正直', '精度', '自体', 'よい'],\n",
       " ['常駐', '先', '上長', 'メンタル', '辛い', 'そう'],\n",
       " ['期日'],\n",
       " ['打ち合わせ', '中', '体調', '悪い'],\n",
       " ['結局', '僕', '凡', 'ミス', '原因'],\n",
       " ['家'],\n",
       " ['仲', '悪い', 'の', 'よう', 'やり取り', 'みんな', '大丈夫', '元気'],\n",
       " ['変化', '気', 'よう', '期待', '損'],\n",
       " ['ドローン', '人間', '事', '何', 'よう', '宣伝', 'ヤツ', 'の', 'マジで', '迷惑', 'ん'],\n",
       " ['案件', '業務', '内容', '統一', '評価基準', '用意', 'の', '難しい'],\n",
       " ['良い', '子', '大人'],\n",
       " ['先月', '案件', '継続', '確認', '月末', 'イレギュラー', '多い', 'の', '単純', '業務', '量', 'なのか'],\n",
       " ['パワポ', '資料', '作り方', '対象', '意図', '不明瞭'],\n",
       " ['夕方', 'ごろ', '検証', '用', 'コード', 'ミス'],\n",
       " ['目的', '変数', '説明', '変数', '相関', '低い', '気'],\n",
       " ['自分', '悲しい', '作業', '前', '時間', '整理', 'ほう', 'いい', '反省'],\n",
       " ['大元', 'これ', '倍', '以上', '発狂'],\n",
       " ['気', 'パン', 'おいしい'],\n",
       " ['先方', '担当者', '上', '報告'],\n",
       " ['担当', '部署', 'さん', '部署', '内', '情報共有'],\n",
       " ['半年', '信頼', 'こと'],\n",
       " ['便利', 'ブックマーク', '嬉しい'],\n",
       " ['技術的', '興味', '嬉しい', 'それ', '事', '〆切', 'かも', '嬉しい'],\n",
       " ['上司', '直接', '言葉', '嬉しい'],\n",
       " ['たくさん', '方', 'おかげ', '社会人', '充実', '時間', 'こと'],\n",
       " ['今日', '勉強', '時間'],\n",
       " ['何', 'ところ', '怖い'],\n",
       " ['ストレス', 'よう', 'いい', 'の'],\n",
       " ['確認', '不足', '迷惑', 'おかけ', '大変', '申し訳'],\n",
       " ['これ', 'がんばり', '否定', 'よう', '気分']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wakati_list = []\n",
    "\n",
    "# 文章の上から順に作成した関数を実行\n",
    "for diary in df['text']:\n",
    "    diary_normalization = get_diary_normalization(diary)\n",
    "    wakati_words = split_words(diary_normalization)\n",
    "    wakati_list.append(wakati_words)\n",
    "    \n",
    "wakati_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA用の辞書作成\n",
    "使用単語をリスト化したものを辞書と呼び、gensimの型の辞書を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 参考URL：https://pira-nino.hatenablog.com/entry/2018/07/29/B%27z%E3%81%AE%E6%AD%8C%E8%A9%9E%E3%82%92Python%E3%81%A8%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%A7%E5%88%86%E6%9E%90%E3%81%97%E3%81%A6%E3%81%BF%E3%81%9F_%E3%80%9CLDA%E7%B7%A8%E3%80%9C\n",
    "# from gensim import corpora\n",
    "\n",
    "dictionary =corpora.Dictionary(wakati_list)\n",
    "\n",
    "# dictionary.filter_extremes(no_below=3, no_above=0.3) で特徴語として相応しくない単語を削除\n",
    "# no_below で指定された数値より下回る頻度で出現した単語は特徴語としてみなされず、また逆に no_above で書かれた割合より多く出現している単語は除かれます。\n",
    "# dictionary.filter_extremes(no_above=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus作成\n",
    "作成した辞書を元に、gensimのLDAの入力の型に文書データを変換\n",
    "doc2bowという関数を使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus=[dictionary.doc2bow(tokens) for tokens in wakati_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDAの学習\n",
    "作成辞書とコーパスを使ってLDAを回す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#トピック数の設定\n",
    "topic_num=2\n",
    "#モデルの学習\n",
    "model = gensim.models.LdaModel(corpus,\n",
    "                               num_topics=topic_num,\n",
    "                               id2word=dictionary,\n",
    "                               random_state=0\n",
    "                              )\n",
    "\n",
    "model.save('lda_test.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC: 0 → 0.015*\"嬉しい\" + 0.015*\"の\" + 0.015*\"気\" + 0.014*\"こと\" + 0.013*\"案件\" + 0.013*\"これ\" + 0.011*\"よう\" + 0.011*\"皆様\" + 0.010*\"いい\" + 0.010*\"確認\"\n",
      "------------------------------------------\n",
      "TOPIC: 1 → 0.017*\"よう\" + 0.013*\"こと\" + 0.012*\"悪い\" + 0.012*\"の\" + 0.012*\"嬉しい\" + 0.012*\"みんな\" + 0.009*\"これ\" + 0.009*\"そう\" + 0.009*\"時間\" + 0.009*\"以上\"\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# model.print_topic(i)でi番のトピックを取得\n",
    "# for文でトピックを出力\n",
    "for i in range(topic_num):\n",
    "    print('TOPIC:', i, '→', model.print_topic(i))\n",
    "    print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "TOPIC 0\n",
      "\n",
      "嬉しい                 0.015400723554193974\n",
      "の                   0.015278492122888565\n",
      "気                   0.014543972909450531\n",
      "こと                  0.01413735467940569\n",
      "案件                  0.013479954563081264\n",
      "これ                  0.013404465280473232\n",
      "よう                  0.011225086636841297\n",
      "皆様                  0.010529246181249619\n",
      "いい                  0.010475586168467999\n",
      "確認                  0.009767688810825348\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TOPIC 1\n",
      "\n",
      "よう                  0.016998596489429474\n",
      "こと                  0.013443437404930592\n",
      "悪い                  0.012392902746796608\n",
      "の                   0.012050390243530273\n",
      "嬉しい                 0.01190117560327053\n",
      "みんな                 0.011848006397485733\n",
      "これ                  0.009221171960234642\n",
      "そう                  0.009022468701004982\n",
      "時間                  0.008878090418875217\n",
      "以上                  0.008712693117558956\n"
     ]
    }
   ],
   "source": [
    "# 上記を見やすく出力\n",
    "# model.show_topic(i)でi番のトピックを取得\n",
    "# http://lab.astamuse.co.jp/entry/try-lda\n",
    "for i in range(topic_num):\n",
    "    print(\"\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"TOPIC {0}\\n\".format(i))\n",
    "    topic = model.show_topic(i)\n",
    "    for t in topic:\n",
    "        print(\"{0:20s}{1}\".format(t[0], t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
