{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tomori kengo\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import neologdn\n",
    "import MeCab\n",
    "\n",
    "import re\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日誌サンプルデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding='utf-8'で上手くいった\n",
    "diary_df = pd.read_csv('SBI_Financial statement_201903.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>国内経済が緩やかに回復するなか、マーケット環境は、米国と中国の貿易摩擦問題に対する警戒感等か...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>このような環境の中、当社業績においては、ホールセールビジネスの拡大、トレーディング収益や金融...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>当社は引続き他社を大きく上回る高いシェアを維持し、35.9％のシェアを獲得。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>先物・オプションの委託個人売買代金シェアは、引き続き高水準を維持。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>投資信託残高の四半期末残高は過去最高を更新し、信託報酬は高水準を維持。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018年4月から2018年6月までの上場会社数は20社。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>同期間のSBI証券引受関与率は100％と 引き続き業界トップ。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  国内経済が緩やかに回復するなか、マーケット環境は、米国と中国の貿易摩擦問題に対する警戒感等か...\n",
       "1  このような環境の中、当社業績においては、ホールセールビジネスの拡大、トレーディング収益や金融...\n",
       "2             当社は引続き他社を大きく上回る高いシェアを維持し、35.9％のシェアを獲得。\n",
       "3                  先物・オプションの委託個人売買代金シェアは、引き続き高水準を維持。\n",
       "4                投資信託残高の四半期末残高は過去最高を更新し、信託報酬は高水準を維持。\n",
       "5                      2018年4月から2018年6月までの上場会社数は20社。\n",
       "6                    同期間のSBI証券引受関与率は100％と 引き続き業界トップ。"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeCabにかける前準備としてneologdn.normalize()を使用して文章全体を正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_diary_normalization(text):\n",
    "    diary_normalization = neologdn.normalize(text)\n",
    "    return diary_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeCab + neologdで形態素解析し、名詞、形容詞原形と動詞原形を抽出しリストに格納\n",
    "##### 参考：https://github.com/kujirahand/book-mlearn-gyomu/blob/master/src/ch4/Doc2Vec/create_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neologd_tagger = MeCab.Tagger('-Ochasen -d C:\\mecab-ipadic-neologd')\n",
    "\n",
    "# 引数のテキストを分かち書きして配列にする\n",
    "# node.surface: 文字のみ取得できる 出力例：同  期間  の  SBI  証券\n",
    "# node.feature: 品詞、原形などの詳細を取得できる:「品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音」の順\n",
    "# よって、「品詞」を取得したい場合はnode.featureをsplit()でリスト型にした後に抽出したい詳細のインデックス番号[0]を指定すればよい\n",
    "# また、動詞や形容詞の「原形（の単語）」を取得したい場合はインデックス番号[6]を指定すればよい\n",
    "\n",
    "def split_words(diary_normalization):\n",
    "    node = neologd_tagger.parseToNode(diary_normalization) #parseだとエラー「 'str' object has no attribute 'feature'」\n",
    "    wakati_words = []\n",
    "    while node is not None:\n",
    "        hinshi = node.feature.split(\",\")[0]\n",
    "        if  hinshi in [\"名詞\"]:\n",
    "            wakati_words.append(node.surface)\n",
    "        elif hinshi in [\"動詞\", \"形容詞\"]:\n",
    "            wakati_words.append(node.feature.split(\",\")[6])\n",
    "        node = node.next\n",
    "    return wakati_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['国内',\n",
       "  '経済',\n",
       "  '緩やか',\n",
       "  '回復',\n",
       "  'する',\n",
       "  'なか',\n",
       "  'マーケット',\n",
       "  '環境',\n",
       "  '米国',\n",
       "  '中国',\n",
       "  '貿易摩擦',\n",
       "  '問題',\n",
       "  '警戒感',\n",
       "  '等',\n",
       "  '一進一退',\n",
       "  '展開'],\n",
       " ['よう',\n",
       "  '環境',\n",
       "  '中',\n",
       "  '当社',\n",
       "  '業績',\n",
       "  'ホールセール',\n",
       "  'ビジネス',\n",
       "  '拡大',\n",
       "  'トレーディング',\n",
       "  '収益',\n",
       "  '金融',\n",
       "  '収益',\n",
       "  '増加',\n",
       "  '前年同期',\n",
       "  '比',\n",
       "  '増収増益',\n",
       "  '達成'],\n",
       " ['当社', '他社', '大きい', '上回る', '高い', 'シェア', '維持', 'する', '35.9%', 'シェア', '獲得'],\n",
       " ['先物', 'オプション', '委託', '個人', '売買代金', 'シェア', '高水準', '維持'],\n",
       " ['投資信託', '残高', '四半期', '末', '残高', '過去最高', '更新', 'する', '信託報酬', '高水準', '維持'],\n",
       " ['2018年', '4月', '2018年', '6月', '上場会社', '数', '20', '社'],\n",
       " ['期間', 'SBI証券', '引受', '関与', '率', '100%', '業界', 'トップ']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wakati_list = []\n",
    "\n",
    "# 文章の上から順に作成した関数を実行\n",
    "for diary in diary_df['text']:\n",
    "    diary_normalization = get_diary_normalization(diary)\n",
    "    wakati_words = split_words(diary_normalization)\n",
    "    wakati_list.append(wakati_words)\n",
    "    \n",
    "wakati_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['国内', '経済', '緩やか', '回復', 'する', 'なか', 'マーケット', '環境', '米国', '中国', '貿易摩擦', '問題', '警戒感', '等', '一進一退', '展開'], tags=[0]),\n",
       " TaggedDocument(words=['よう', '環境', '中', '当社', '業績', 'ホールセール', 'ビジネス', '拡大', 'トレーディング', '収益', '金融', '収益', '増加', '前年同期', '比', '増収増益', '達成'], tags=[1]),\n",
       " TaggedDocument(words=['当社', '他社', '大きい', '上回る', '高い', 'シェア', '維持', 'する', '35.9%', 'シェア', '獲得'], tags=[2]),\n",
       " TaggedDocument(words=['先物', 'オプション', '委託', '個人', '売買代金', 'シェア', '高水準', '維持'], tags=[3]),\n",
       " TaggedDocument(words=['投資信託', '残高', '四半期', '末', '残高', '過去最高', '更新', 'する', '信託報酬', '高水準', '維持'], tags=[4]),\n",
       " TaggedDocument(words=['2018年', '4月', '2018年', '6月', '上場会社', '数', '20', '社'], tags=[5]),\n",
       " TaggedDocument(words=['期間', 'SBI証券', '引受', '関与', '率', '100%', '業界', 'トップ'], tags=[6])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainings_wakati_list = [TaggedDocument(words = wakati_list[i], tags = [i]) for i in range(len(diary_df))]\n",
    "trainings_wakati_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MUFGのFinancial Statementで学習したモデルのロード\n",
    "model = Doc2Vec.load('MFUG_Financial_statement.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5842480364298264"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習したモデルを使って、「今回の文書」同士の類似度を調べる\n",
    "model.docvecs.similarity_unseen_docs(model, wakati_list[0], wakati_list[1], alpha=1, min_alpha=0.0001, steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39001291609943156"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.similarity_unseen_docs(model, wakati_list[0], wakati_list[5], alpha=1, min_alpha=0.0001, steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16607250485776925"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 任意の文書間の類似度を調べる\n",
    "# 引数は学習した文章(MFUG)のtagを指定\n",
    "# このtagは今回の文章ではなく学習した文章(MFUG)のtag\n",
    "model.docvecs.similarity(0,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 0.1660725176334381),\n",
       " (9, 0.13208238780498505),\n",
       " (13, 0.1253616213798523),\n",
       " (5, 0.1136433333158493),\n",
       " (7, 0.10438272356987),\n",
       " (16, 0.10097911953926086),\n",
       " (2, 0.10084007680416107),\n",
       " (14, 0.0999135822057724),\n",
       " (11, 0.09619168192148209),\n",
       " (1, 0.08139412105083466)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習文書の中で、指定したidの文書と類似度の高い文書を調べる\n",
    "# 引数は学習した文章(MFUG)のtagを指定\n",
    "# このtagは今回の文章ではなく学習した文章のtag\n",
    "model.docvecs.most_similar(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 参考：https://qiita.com/asian373asian/items/1be1bec7f2297b8326cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
