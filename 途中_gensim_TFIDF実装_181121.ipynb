{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# bag of wordsを作成するためのライブラリ\n",
    "from gensim import corpora, matutils\n",
    "# TF-IDFを作成するためのライブラリ\n",
    "from gensim import models\n",
    "\n",
    "# progress\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surface</th>\n",
       "      <th>yomi</th>\n",
       "      <th>original</th>\n",
       "      <th>type</th>\n",
       "      <th>katsuyoukei</th>\n",
       "      <th>katsuyougata</th>\n",
       "      <th>内容</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neologd</td>\n",
       "      <td>ネオログディー</td>\n",
       "      <td>NEologd</td>\n",
       "      <td>名詞-固有名詞-一般</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neologd_tagger.parse(text)で各単語の原形、品詞などが1行で連続して...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>記号-一般</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neologd_tagger.parse(text)で各単語の原形、品詞などが1行で連続して...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tagger</td>\n",
       "      <td>タガー</td>\n",
       "      <td>tagger</td>\n",
       "      <td>名詞-固有名詞-一般</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neologd_tagger.parse(text)で各単語の原形、品詞などが1行で連続して...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>記号-一般</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neologd_tagger.parse(text)で各単語の原形、品詞などが1行で連続して...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parse</td>\n",
       "      <td>パース</td>\n",
       "      <td>Parse</td>\n",
       "      <td>名詞-固有名詞-一般</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neologd_tagger.parse(text)で各単語の原形、品詞などが1行で連続して...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   surface     yomi original        type katsuyoukei katsuyougata  \\\n",
       "0  neologd  ネオログディー  NEologd  名詞-固有名詞-一般         NaN          NaN   \n",
       "1        _        _        _       記号-一般         NaN          NaN   \n",
       "2   tagger      タガー   tagger  名詞-固有名詞-一般         NaN          NaN   \n",
       "3        .        .        .       記号-一般         NaN          NaN   \n",
       "4    parse      パース    Parse  名詞-固有名詞-一般         NaN          NaN   \n",
       "\n",
       "                                                  内容  \n",
       "0  neologd_tagger.parse(text)で各単語の原形、品詞などが1行で連続して...  \n",
       "1  neologd_tagger.parse(text)で各単語の原形、品詞などが1行で連続して...  \n",
       "2  neologd_tagger.parse(text)で各単語の原形、品詞などが1行で連続して...  \n",
       "3  neologd_tagger.parse(text)で各単語の原形、品詞などが1行で連続して...  \n",
       "4  neologd_tagger.parse(text)で各単語の原形、品詞などが1行で連続して...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('all_text_wakatigaki.csv',encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>内容</th>\n",
       "      <th>wakati_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neologd_tagger.parse(text)で各単語の原形、品詞などが1行で連続して...</td>\n",
       "      <td>NEologd _ tagger . Parse ( text ) で 各 単語 の 原形 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>まずはparse()で分かち書きした単語群は1つの文字列型になっているので「\\n」で区切り、...</td>\n",
       "      <td>まずは Parse () で 分かち書き する た 単語 群 は 1つ の 文字列 型 に ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>原形、品詞などの間には「\\t」が、分かち書きされた単語と単語の区切りには「\\n」が表示される</td>\n",
       "      <td>原形 、 品詞 など の 間 に は 「 \\ t 」 が 、 分かち書き する れる た 単...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>次に邪魔な文字「\\t」を省きたい。よって、split('\\t')を使用したいがwakatid...</td>\n",
       "      <td>次に 邪魔 だ 文字 「\\ t 」 を 省く たい 。 よって 、 split ('\\ t ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  内容  \\\n",
       "0  neologd_tagger.parse(text)で各単語の原形、品詞などが1行で連続して...   \n",
       "1  まずはparse()で分かち書きした単語群は1つの文字列型になっているので「\\n」で区切り、...   \n",
       "2     原形、品詞などの間には「\\t」が、分かち書きされた単語と単語の区切りには「\\n」が表示される   \n",
       "3  次に邪魔な文字「\\t」を省きたい。よって、split('\\t')を使用したいがwakatid...   \n",
       "\n",
       "                                        wakati_words  \n",
       "0  NEologd _ tagger . Parse ( text ) で 各 単語 の 原形 ...  \n",
       "1  まずは Parse () で 分かち書き する た 単語 群 は 1つ の 文字列 型 に ...  \n",
       "2  原形 、 品詞 など の 間 に は 「 \\ t 」 が 、 分かち書き する れる た 単...  \n",
       "3  次に 邪魔 だ 文字 「\\ t 」 を 省く たい 。 よって 、 split ('\\ t ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.groupbyでグループ化された[内容]毎の[original]に対して文字列のリスト（配列）を一つの文字列に連結する「join」関数を各行（各要素）に対して実行する\n",
    "# join関数の使い方: '間に挿入する文字列'.join([連結したい文字列のリスト])\n",
    "text_wakati_df = df.groupby('内容')['original'].apply(lambda x: ' '.join(map(str, x))).reset_index()\n",
    "\n",
    "# カラム名を適切な名前に変更\n",
    "text_wakati_df = text_wakati_df.rename(columns={'original':'wakati_words'})\n",
    "\n",
    "# 確認\n",
    "text_wakati_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NEologd',\n",
       "  '_',\n",
       "  'tagger',\n",
       "  '.',\n",
       "  'Parse',\n",
       "  '(',\n",
       "  'text',\n",
       "  ')',\n",
       "  'で',\n",
       "  '各',\n",
       "  '単語',\n",
       "  'の',\n",
       "  '原形',\n",
       "  '、',\n",
       "  '品詞',\n",
       "  'など',\n",
       "  'が',\n",
       "  '1',\n",
       "  '行',\n",
       "  'で',\n",
       "  '連続',\n",
       "  'する',\n",
       "  'て',\n",
       "  '表示',\n",
       "  'する',\n",
       "  'れる'],\n",
       " ['まずは',\n",
       "  'Parse',\n",
       "  '()',\n",
       "  'で',\n",
       "  '分かち書き',\n",
       "  'する',\n",
       "  'た',\n",
       "  '単語',\n",
       "  '群',\n",
       "  'は',\n",
       "  '1つ',\n",
       "  'の',\n",
       "  '文字列',\n",
       "  '型',\n",
       "  'に',\n",
       "  'なる',\n",
       "  'て',\n",
       "  'いる',\n",
       "  'ので',\n",
       "  '「',\n",
       "  '\\\\',\n",
       "  'n',\n",
       "  '」',\n",
       "  'で',\n",
       "  '区切る',\n",
       "  '、',\n",
       "  'リスト',\n",
       "  '型',\n",
       "  'に',\n",
       "  'する'],\n",
       " ['原形',\n",
       "  '、',\n",
       "  '品詞',\n",
       "  'など',\n",
       "  'の',\n",
       "  '間',\n",
       "  'に',\n",
       "  'は',\n",
       "  '「',\n",
       "  '\\\\',\n",
       "  't',\n",
       "  '」',\n",
       "  'が',\n",
       "  '、',\n",
       "  '分かち書き',\n",
       "  'する',\n",
       "  'れる',\n",
       "  'た',\n",
       "  '単語',\n",
       "  'と',\n",
       "  '単語',\n",
       "  'の',\n",
       "  '区切り',\n",
       "  'に',\n",
       "  'は',\n",
       "  '「',\n",
       "  '\\\\',\n",
       "  'n',\n",
       "  '」',\n",
       "  'が',\n",
       "  '表示',\n",
       "  'する',\n",
       "  'れる'],\n",
       " ['次に',\n",
       "  '邪魔',\n",
       "  'だ',\n",
       "  '文字',\n",
       "  '「\\\\',\n",
       "  't',\n",
       "  '」',\n",
       "  'を',\n",
       "  '省く',\n",
       "  'たい',\n",
       "  '。',\n",
       "  'よって',\n",
       "  '、',\n",
       "  'split',\n",
       "  \"('\\\\\",\n",
       "  't',\n",
       "  \"')\",\n",
       "  'を',\n",
       "  '使用',\n",
       "  'する',\n",
       "  'たい',\n",
       "  'が',\n",
       "  'wakatid',\n",
       "  '_',\n",
       "  'text',\n",
       "  'は',\n",
       "  '文字列',\n",
       "  '型',\n",
       "  'で',\n",
       "  'は',\n",
       "  'ない',\n",
       "  'リスト',\n",
       "  '型',\n",
       "  'の',\n",
       "  'ため',\n",
       "  'split',\n",
       "  '関数',\n",
       "  'を',\n",
       "  '使用',\n",
       "  'できる',\n",
       "  'ない']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wakati_words_list = text_wakati_df['wakati_words'].str.split().tolist()\n",
    "wakati_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bag of words作成準備\n",
    "# 全種類の単語のIDを付与した辞書の作成\n",
    "corpus_dic = corpora.Dictionary(wakati_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 各文章の単語リストをコーパス（辞書の単語IDと単語の出現回数）リストに変換\n",
    "# 辞書オブジェクトからdoc2bow関数を呼び出し、引数に単語リストを指定することで辞書に基づいたコーパスを取得できる\n",
    "# コーパスとは、文章に含まれる単語を、辞書の単語IDと単語の出現回数の組み合わせに変換したもの\n",
    "# bag of wordsに変換するときは変換元のデータ番号が行列の行番号に、単語IDの番号が行列の列番号に、出現回数が行列の値に該当\n",
    "corpus_list = [corpus_dic.doc2bow(wakati) for wakati in wakati_words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-IDFモデルの作成\n",
    "# models.TfidfModel関数はコーパスリストをTF-IDF値に変換するオブジェクトを生成する関数\n",
    "# 引数にコーパスリストを指定し、normalize引数をTrueにするとTF-IDF値をL2ノルムによって正規化した値に変換するオブジェクトになる\n",
    "tfidf_model = models.TfidfModel(corpus_list, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpusにTF-IDFを適用\n",
    "corpus_list_tfidf = tfidf_model[corpus_list]\n",
    "wakati_tfidf_matrix = matutils.corpus2csc(corpus_list_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.29477648  0.          0.          0.        ]\n",
      " [ 0.29477648  0.          0.          0.        ]\n",
      " [ 0.29477648  0.          0.          0.        ]\n",
      " [ 0.29477648  0.          0.          0.        ]\n",
      " [ 0.29477648  0.          0.          0.        ]\n",
      " [ 0.14738824  0.14113122  0.          0.        ]\n",
      " [ 0.14738824  0.          0.          0.07687628]\n",
      " [ 0.29477648  0.          0.          0.        ]\n",
      " [ 0.14738824  0.          0.          0.07687628]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.06117165  0.          0.13333597  0.03190654]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.14738824  0.14113122  0.          0.        ]\n",
      " [ 0.12234329  0.11714949  0.          0.03190654]\n",
      " [ 0.14738824  0.          0.16063123  0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.14738824  0.          0.32126245  0.        ]\n",
      " [ 0.06117165  0.05857475  0.13333597  0.        ]\n",
      " [ 0.14738824  0.          0.16063123  0.        ]\n",
      " [ 0.29477648  0.          0.          0.        ]\n",
      " [ 0.14738824  0.          0.16063123  0.        ]\n",
      " [ 0.29477648  0.          0.          0.        ]\n",
      " [ 0.14738824  0.          0.16063123  0.        ]\n",
      " [ 0.29477648  0.          0.          0.        ]\n",
      " [ 0.          0.28226243  0.          0.        ]\n",
      " [ 0.          0.28226243  0.          0.        ]\n",
      " [ 0.          0.14113122  0.32126245  0.        ]\n",
      " [ 0.          0.14113122  0.16063123  0.        ]\n",
      " [ 0.          0.14113122  0.32126245  0.        ]\n",
      " [ 0.          0.05857475  0.13333597  0.03190654]\n",
      " [ 0.          0.28226243  0.          0.        ]\n",
      " [ 0.          0.14113122  0.16063123  0.        ]\n",
      " [ 0.          0.28226243  0.          0.        ]\n",
      " [ 0.          0.28226243  0.32126245  0.        ]\n",
      " [ 0.          0.28226243  0.          0.        ]\n",
      " [ 0.          0.05857475  0.13333597  0.06381308]\n",
      " [ 0.          0.28226243  0.          0.        ]\n",
      " [ 0.          0.14113122  0.          0.07687628]\n",
      " [ 0.          0.14113122  0.16063123  0.        ]\n",
      " [ 0.          0.28226243  0.          0.        ]\n",
      " [ 0.          0.28226243  0.          0.15375257]\n",
      " [ 0.          0.14113122  0.          0.07687628]\n",
      " [ 0.          0.28226243  0.          0.        ]\n",
      " [ 0.          0.          0.16063123  0.15375257]\n",
      " [ 0.          0.          0.32126245  0.        ]\n",
      " [ 0.          0.          0.32126245  0.        ]\n",
      " [ 0.          0.          0.32126245  0.        ]\n",
      " [ 0.          0.          0.          0.15375257]\n",
      " [ 0.          0.          0.          0.15375257]\n",
      " [ 0.          0.          0.          0.30750514]\n",
      " [ 0.          0.          0.          0.15375257]\n",
      " [ 0.          0.          0.          0.15375257]\n",
      " [ 0.          0.          0.          0.15375257]\n",
      " [ 0.          0.          0.          0.30750514]\n",
      " [ 0.          0.          0.          0.15375257]\n",
      " [ 0.          0.          0.          0.15375257]\n",
      " [ 0.          0.          0.          0.15375257]\n",
      " [ 0.          0.          0.          0.30750514]\n",
      " [ 0.          0.          0.          0.15375257]\n",
      " [ 0.          0.          0.          0.46125771]\n",
      " [ 0.          0.          0.          0.30750514]\n",
      " [ 0.          0.          0.          0.15375257]\n",
      " [ 0.          0.          0.          0.15375257]\n",
      " [ 0.          0.          0.          0.15375257]\n",
      " [ 0.          0.          0.          0.15375257]\n",
      " [ 0.          0.          0.          0.15375257]]\n"
     ]
    }
   ],
   "source": [
    "print(wakati_tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.29477648  0.29477648  0.29477648  0.29477648  0.29477648  0.14738824\n",
      "   0.14738824  0.29477648  0.14738824  0.          0.06117165  0.\n",
      "   0.14738824  0.12234329  0.14738824  0.          0.14738824  0.06117165\n",
      "   0.14738824  0.29477648  0.14738824  0.29477648  0.14738824  0.29477648\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.14113122\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.14113122  0.11714949  0.          0.          0.          0.05857475\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.28226243  0.28226243  0.14113122  0.14113122  0.14113122  0.05857475\n",
      "   0.28226243  0.14113122  0.28226243  0.28226243  0.28226243  0.05857475\n",
      "   0.28226243  0.14113122  0.14113122  0.28226243  0.28226243  0.14113122\n",
      "   0.28226243  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.13333597  0.          0.          0.\n",
      "   0.16063123  0.          0.32126245  0.13333597  0.16063123  0.\n",
      "   0.16063123  0.          0.16063123  0.          0.          0.\n",
      "   0.32126245  0.16063123  0.32126245  0.13333597  0.          0.16063123\n",
      "   0.          0.32126245  0.          0.13333597  0.          0.\n",
      "   0.16063123  0.          0.          0.          0.          0.16063123\n",
      "   0.32126245  0.32126245  0.32126245  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.07687628  0.          0.07687628  0.          0.03190654  0.          0.\n",
      "   0.03190654  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.03190654  0.          0.          0.          0.\n",
      "   0.          0.06381308  0.          0.07687628  0.          0.\n",
      "   0.15375257  0.07687628  0.          0.15375257  0.          0.          0.\n",
      "   0.15375257  0.15375257  0.30750514  0.15375257  0.15375257  0.15375257\n",
      "   0.30750514  0.15375257  0.15375257  0.15375257  0.30750514  0.15375257\n",
      "   0.46125771  0.30750514  0.15375257  0.15375257  0.15375257  0.15375257\n",
      "   0.15375257]]\n"
     ]
    }
   ],
   "source": [
    "print(wakati_tfidf_matrix.toarray().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 0.294776477035\n",
      ") 0.294776477035\n",
      ". 0.294776477035\n",
      "1 0.294776477035\n",
      "NEologd 0.294776477035\n",
      "Parse 0.147388238518\n",
      "_ 0.147388238518\n",
      "tagger 0.294776477035\n",
      "text 0.147388238518\n",
      "が 0.0611716459375\n",
      "て 0.147388238518\n",
      "で 0.122343291875\n",
      "など 0.147388238518\n",
      "れる 0.147388238518\n",
      "単語 0.0611716459375\n",
      "原形 0.147388238518\n",
      "各 0.294776477035\n",
      "品詞 0.147388238518\n",
      "行 0.294776477035\n",
      "表示 0.147388238518\n",
      "連続 0.294776477035\n",
      "Parse 0.141131216478\n",
      "て 0.141131216478\n",
      "で 0.117149494315\n",
      "単語 0.0585747471574\n",
      "() 0.282262432957\n",
      "1つ 0.282262432957\n",
      "\\ 0.141131216478\n",
      "n 0.141131216478\n",
      "「 0.141131216478\n",
      "」 0.0585747471574\n",
      "いる 0.282262432957\n",
      "た 0.141131216478\n",
      "なる 0.282262432957\n",
      "に 0.282262432957\n",
      "ので 0.282262432957\n",
      "は 0.0585747471574\n",
      "まずは 0.282262432957\n",
      "リスト 0.141131216478\n",
      "分かち書き 0.141131216478\n",
      "区切る 0.282262432957\n",
      "型 0.282262432957\n",
      "文字列 0.141131216478\n",
      "群 0.282262432957\n",
      "が 0.133335965657\n",
      "など 0.160631227164\n",
      "れる 0.321262454328\n",
      "単語 0.133335965657\n",
      "原形 0.160631227164\n",
      "品詞 0.160631227164\n",
      "表示 0.160631227164\n",
      "\\ 0.321262454328\n",
      "n 0.160631227164\n",
      "「 0.321262454328\n",
      "」 0.133335965657\n",
      "た 0.160631227164\n",
      "に 0.321262454328\n",
      "は 0.133335965657\n",
      "分かち書き 0.160631227164\n",
      "t 0.160631227164\n",
      "と 0.321262454328\n",
      "区切り 0.321262454328\n",
      "間 0.321262454328\n",
      "_ 0.0768762846781\n",
      "text 0.0768762846781\n",
      "が 0.0319065409467\n",
      "で 0.0319065409467\n",
      "」 0.0319065409467\n",
      "は 0.0638130818933\n",
      "リスト 0.0768762846781\n",
      "型 0.153752569356\n",
      "文字列 0.0768762846781\n",
      "t 0.153752569356\n",
      "') 0.153752569356\n",
      "('\\ 0.153752569356\n",
      "split 0.307505138713\n",
      "wakatid 0.153752569356\n",
      "。 0.153752569356\n",
      "「\\ 0.153752569356\n",
      "たい 0.307505138713\n",
      "ため 0.153752569356\n",
      "だ 0.153752569356\n",
      "できる 0.153752569356\n",
      "ない 0.307505138713\n",
      "よって 0.153752569356\n",
      "を 0.461257708069\n",
      "使用 0.307505138713\n",
      "文字 0.153752569356\n",
      "次に 0.153752569356\n",
      "省く 0.153752569356\n",
      "邪魔 0.153752569356\n",
      "関数 0.153752569356\n"
     ]
    }
   ],
   "source": [
    "for i in corpus_list_tfidf:\n",
    "    for j in i:\n",
    "        print(corpus_dic[j[0]], j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 2),\n",
       "  (12, 1),\n",
       "  (13, 2),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1)],\n",
       " [(5, 1),\n",
       "  (9, 1),\n",
       "  (11, 2),\n",
       "  (12, 1),\n",
       "  (13, 2),\n",
       "  (15, 1),\n",
       "  (17, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 2),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 2),\n",
       "  (41, 1),\n",
       "  (42, 1)],\n",
       " [(9, 2),\n",
       "  (10, 2),\n",
       "  (11, 2),\n",
       "  (14, 1),\n",
       "  (15, 2),\n",
       "  (16, 2),\n",
       "  (17, 2),\n",
       "  (18, 1),\n",
       "  (20, 1),\n",
       "  (22, 1),\n",
       "  (26, 2),\n",
       "  (27, 1),\n",
       "  (28, 2),\n",
       "  (29, 2),\n",
       "  (31, 1),\n",
       "  (33, 2),\n",
       "  (35, 2),\n",
       "  (38, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1)],\n",
       " [(6, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (13, 1),\n",
       "  (15, 1),\n",
       "  (29, 1),\n",
       "  (35, 2),\n",
       "  (37, 1),\n",
       "  (40, 2),\n",
       "  (41, 1),\n",
       "  (43, 2),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 2),\n",
       "  (50, 1),\n",
       "  (51, 1),\n",
       "  (52, 1),\n",
       "  (53, 2),\n",
       "  (54, 1),\n",
       "  (55, 1),\n",
       "  (56, 1),\n",
       "  (57, 2),\n",
       "  (58, 1),\n",
       "  (59, 3),\n",
       "  (60, 2),\n",
       "  (61, 1),\n",
       "  (62, 1),\n",
       "  (63, 1),\n",
       "  (64, 1),\n",
       "  (65, 1)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(': 0, ')': 1, '.': 2, '1': 3, 'NEologd': 4, 'Parse': 5, '_': 6, 'tagger': 7, 'text': 8, '、': 9, 'が': 10, 'する': 11, 'て': 12, 'で': 13, 'など': 14, 'の': 15, 'れる': 16, '単語': 17, '原形': 18, '各': 19, '品詞': 20, '行': 21, '表示': 22, '連続': 23, '()': 24, '1つ': 25, '\\\\': 26, 'n': 27, '「': 28, '」': 29, 'いる': 30, 'た': 31, 'なる': 32, 'に': 33, 'ので': 34, 'は': 35, 'まずは': 36, 'リスト': 37, '分かち書き': 38, '区切る': 39, '型': 40, '文字列': 41, '群': 42, 't': 43, 'と': 44, '区切り': 45, '間': 46, \"')\": 47, \"('\\\\\": 48, 'split': 49, 'wakatid': 50, '。': 51, '「\\\\': 52, 'たい': 53, 'ため': 54, 'だ': 55, 'できる': 56, 'ない': 57, 'よって': 58, 'を': 59, '使用': 60, '文字': 61, '次に': 62, '省く': 63, '邪魔': 64, '関数': 65}\n"
     ]
    }
   ],
   "source": [
    "print(corpus_dic.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_tfidf = [] # id -> 単語表示に変えた文書ごとのTF-IDF\n",
    "for doc in corpus_list_tfidf:\n",
    "    text_tfidf = []\n",
    "    for word in doc:\n",
    "        text_tfidf.append([corpus_dic[word[0]],word[1]])\n",
    "    texts_tfidf.append(text_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['(', 0.29477647703525461], [')', 0.29477647703525461], ['.', 0.29477647703525461], ['1', 0.29477647703525461], ['NEologd', 0.29477647703525461], ['Parse', 0.14738823851762731], ['_', 0.14738823851762731], ['tagger', 0.29477647703525461], ['text', 0.14738823851762731], ['が', 0.061171645937469793], ['て', 0.14738823851762731], ['で', 0.12234329187493959], ['など', 0.14738823851762731], ['れる', 0.14738823851762731], ['単語', 0.061171645937469793], ['原形', 0.14738823851762731], ['各', 0.29477647703525461], ['品詞', 0.14738823851762731], ['行', 0.29477647703525461], ['表示', 0.14738823851762731], ['連続', 0.29477647703525461]]\n",
      "[['Parse', 0.14113121647834134], ['て', 0.14113121647834134], ['で', 0.11714949431470387], ['単語', 0.058574747157351933], ['()', 0.28226243295668269], ['1つ', 0.28226243295668269], ['\\\\', 0.14113121647834134], ['n', 0.14113121647834134], ['「', 0.14113121647834134], ['」', 0.058574747157351933], ['いる', 0.28226243295668269], ['た', 0.14113121647834134], ['なる', 0.28226243295668269], ['に', 0.28226243295668269], ['ので', 0.28226243295668269], ['は', 0.058574747157351933], ['まずは', 0.28226243295668269], ['リスト', 0.14113121647834134], ['分かち書き', 0.14113121647834134], ['区切る', 0.28226243295668269], ['型', 0.28226243295668269], ['文字列', 0.14113121647834134], ['群', 0.28226243295668269]]\n",
      "[['が', 0.13333596565662129], ['など', 0.16063122716417402], ['れる', 0.32126245432834805], ['単語', 0.13333596565662129], ['原形', 0.16063122716417402], ['品詞', 0.16063122716417402], ['表示', 0.16063122716417402], ['\\\\', 0.32126245432834805], ['n', 0.16063122716417402], ['「', 0.32126245432834805], ['」', 0.13333596565662129], ['た', 0.16063122716417402], ['に', 0.32126245432834805], ['は', 0.13333596565662129], ['分かち書き', 0.16063122716417402], ['t', 0.16063122716417402], ['と', 0.32126245432834805], ['区切り', 0.32126245432834805], ['間', 0.32126245432834805]]\n",
      "[['_', 0.076876284678144183], ['text', 0.076876284678144183], ['が', 0.031906540946665451], ['で', 0.031906540946665451], ['」', 0.031906540946665451], ['は', 0.063813081893330903], ['リスト', 0.076876284678144183], ['型', 0.15375256935628837], ['文字列', 0.076876284678144183], ['t', 0.15375256935628837], [\"')\", 0.15375256935628837], [\"('\\\\\", 0.15375256935628837], ['split', 0.30750513871257673], ['wakatid', 0.15375256935628837], ['。', 0.15375256935628837], ['「\\\\', 0.15375256935628837], ['たい', 0.30750513871257673], ['ため', 0.15375256935628837], ['だ', 0.15375256935628837], ['できる', 0.15375256935628837], ['ない', 0.30750513871257673], ['よって', 0.15375256935628837], ['を', 0.46125770806886512], ['使用', 0.30750513871257673], ['文字', 0.15375256935628837], ['次に', 0.15375256935628837], ['省く', 0.15375256935628837], ['邪魔', 0.15375256935628837], ['関数', 0.15375256935628837]]\n"
     ]
    }
   ],
   "source": [
    "for text in texts_tfidf:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " 'text',\n",
       " 'が',\n",
       " 'で',\n",
       " '」',\n",
       " 'は',\n",
       " 'リスト',\n",
       " '型',\n",
       " '文字列',\n",
       " 't',\n",
       " \"')\",\n",
       " \"('\\\\\",\n",
       " 'split',\n",
       " 'wakatid',\n",
       " '。',\n",
       " '「\\\\',\n",
       " 'たい',\n",
       " 'ため',\n",
       " 'だ',\n",
       " 'できる',\n",
       " 'ない',\n",
       " 'よって',\n",
       " 'を',\n",
       " '使用',\n",
       " '文字',\n",
       " '次に',\n",
       " '省く',\n",
       " '邪魔',\n",
       " '関数']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for doc in corpus_list_tfidf:\n",
    "    text_tfidf = []\n",
    "    for word in doc:\n",
    "        text_tfidf.append(corpus_dic[word[0]])\n",
    "                           \n",
    "text_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf_df = pd.DataFrame(wakati_tfidf_matrix.toarray().T, index=text_wakati_df['内容'],  columns=wakati_words_list)\n",
    "#tfidf_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
