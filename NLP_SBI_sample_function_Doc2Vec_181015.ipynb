{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tomori kengo\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import neologdn\n",
    "import MeCab\n",
    "\n",
    "import re\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日誌サンプルデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding='utf-8'で上手くいった\n",
    "diary_df = pd.read_csv('SBI_Financial statement_201903.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>国内経済が緩やかに回復するなか、マーケット環境は、米国と中国の貿易摩擦問題に対する警戒感等か...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>このような環境の中、当社業績においては、ホールセールビジネスの拡大、トレーディング収益や金融...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>当社は引続き他社を大きく上回る高いシェアを維持し、35.9％のシェアを獲得。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>先物・オプションの委託個人売買代金シェアは、引き続き高水準を維持。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>投資信託残高の四半期末残高は過去最高を更新し、信託報酬は高水準を維持。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018年4月から2018年6月までの上場会社数は20社。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>同期間のSBI証券引受関与率は100％と 引き続き業界トップ。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  国内経済が緩やかに回復するなか、マーケット環境は、米国と中国の貿易摩擦問題に対する警戒感等か...\n",
       "1  このような環境の中、当社業績においては、ホールセールビジネスの拡大、トレーディング収益や金融...\n",
       "2             当社は引続き他社を大きく上回る高いシェアを維持し、35.9％のシェアを獲得。\n",
       "3                  先物・オプションの委託個人売買代金シェアは、引き続き高水準を維持。\n",
       "4                投資信託残高の四半期末残高は過去最高を更新し、信託報酬は高水準を維持。\n",
       "5                      2018年4月から2018年6月までの上場会社数は20社。\n",
       "6                    同期間のSBI証券引受関与率は100％と 引き続き業界トップ。"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeCabにかける前準備としてneologdn.normalize()を使用して文章全体を正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_diary_normalization(text):\n",
    "    diary_normalization = neologdn.normalize(text)\n",
    "    return diary_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeCab + neologdで形態素解析し、名詞、形容詞原形と動詞原形を抽出しリストに格納\n",
    "##### 参考：https://github.com/kujirahand/book-mlearn-gyomu/blob/master/src/ch4/Doc2Vec/create_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neologd_tagger = MeCab.Tagger('-Ochasen -d C:\\mecab-ipadic-neologd')\n",
    "\n",
    "# 引数のテキストを分かち書きして配列にする\n",
    "# node.surface: 文字のみ取得できる 出力例：同  期間  の  SBI  証券\n",
    "# node.feature: 品詞、原形などの詳細を取得できる:「品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音」の順\n",
    "# よって、「品詞」を取得したい場合はnode.featureをsplit()でリスト型にした後に抽出したい詳細のインデックス番号[0]を指定すればよい\n",
    "# また、動詞や形容詞の「原形（の単語）」を取得したい場合はインデックス番号[6]を指定すればよい\n",
    "\n",
    "def split_words(diary_normalization):\n",
    "    node = neologd_tagger.parseToNode(diary_normalization) #parseだとエラー「 'str' object has no attribute 'feature'」\n",
    "    wakati_words = []\n",
    "    while node is not None:\n",
    "        hinshi = node.feature.split(\",\")[0]\n",
    "        if  hinshi in [\"名詞\"]:\n",
    "            wakati_words.append(node.surface)\n",
    "        elif hinshi in [\"動詞\", \"形容詞\"]:\n",
    "            wakati_words.append(node.feature.split(\",\")[6])\n",
    "        node = node.next\n",
    "    return wakati_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['国内',\n",
       "  '経済',\n",
       "  '緩やか',\n",
       "  '回復',\n",
       "  'する',\n",
       "  'なか',\n",
       "  'マーケット',\n",
       "  '環境',\n",
       "  '米国',\n",
       "  '中国',\n",
       "  '貿易摩擦',\n",
       "  '問題',\n",
       "  '警戒感',\n",
       "  '等',\n",
       "  '一進一退',\n",
       "  '展開'],\n",
       " ['よう',\n",
       "  '環境',\n",
       "  '中',\n",
       "  '当社',\n",
       "  '業績',\n",
       "  'ホールセール',\n",
       "  'ビジネス',\n",
       "  '拡大',\n",
       "  'トレーディング',\n",
       "  '収益',\n",
       "  '金融',\n",
       "  '収益',\n",
       "  '増加',\n",
       "  '前年同期',\n",
       "  '比',\n",
       "  '増収増益',\n",
       "  '達成'],\n",
       " ['当社', '他社', '大きい', '上回る', '高い', 'シェア', '維持', 'する', '35.9%', 'シェア', '獲得'],\n",
       " ['先物', 'オプション', '委託', '個人', '売買代金', 'シェア', '高水準', '維持'],\n",
       " ['投資信託', '残高', '四半期', '末', '残高', '過去最高', '更新', 'する', '信託報酬', '高水準', '維持'],\n",
       " ['2018年', '4月', '2018年', '6月', '上場会社', '数', '20', '社'],\n",
       " ['期間', 'SBI証券', '引受', '関与', '率', '100%', '業界', 'トップ']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wakati_list = []\n",
    "\n",
    "# 文章の上から順に作成した関数を実行\n",
    "for diary in diary_df['text']:\n",
    "    diary_normalization = get_diary_normalization(diary)\n",
    "    wakati_words = split_words(diary_normalization)\n",
    "    wakati_list.append(wakati_words)\n",
    "    \n",
    "wakati_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['国内', '経済', '緩やか', '回復', 'する', 'なか', 'マーケット', '環境', '米国', '中国', '貿易摩擦', '問題', '警戒感', '等', '一進一退', '展開'], tags=[0]),\n",
       " TaggedDocument(words=['よう', '環境', '中', '当社', '業績', 'ホールセール', 'ビジネス', '拡大', 'トレーディング', '収益', '金融', '収益', '増加', '前年同期', '比', '増収増益', '達成'], tags=[1]),\n",
       " TaggedDocument(words=['当社', '他社', '大きい', '上回る', '高い', 'シェア', '維持', 'する', '35.9%', 'シェア', '獲得'], tags=[2]),\n",
       " TaggedDocument(words=['先物', 'オプション', '委託', '個人', '売買代金', 'シェア', '高水準', '維持'], tags=[3]),\n",
       " TaggedDocument(words=['投資信託', '残高', '四半期', '末', '残高', '過去最高', '更新', 'する', '信託報酬', '高水準', '維持'], tags=[4]),\n",
       " TaggedDocument(words=['2018年', '4月', '2018年', '6月', '上場会社', '数', '20', '社'], tags=[5]),\n",
       " TaggedDocument(words=['期間', 'SBI証券', '引受', '関与', '率', '100%', '業界', 'トップ'], tags=[6])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainings_wakati_list = [TaggedDocument(words = wakati_list[i], tags = [i]) for i in range(len(diary_df))]\n",
    "trainings_wakati_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tomori kengo\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(documents=trainings_wakati_list, size=100, min_count=1, dm=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.58307229e-03 -2.87590036e-03  1.84542802e-03  3.79760226e-04\n",
      " -3.63458367e-03  1.62870798e-03  2.90631113e-04 -3.76525009e-03\n",
      "  4.19726013e-04  7.41325435e-04 -1.40767696e-03  3.44016519e-03\n",
      " -3.48104659e-04  4.07948624e-03 -4.62286454e-03  4.67364630e-03\n",
      " -4.97925142e-03 -1.16653298e-03  1.45586743e-03 -4.39416012e-03\n",
      " -9.75691772e-04  2.77430099e-03  3.29152821e-03  1.62896793e-03\n",
      " -1.96863874e-03 -4.33021598e-03 -3.45873134e-03  3.67706106e-03\n",
      " -1.87462289e-03 -7.34648711e-05 -1.94591575e-03 -4.40182025e-03\n",
      "  2.14161072e-03 -4.20969538e-03 -1.44155475e-03 -2.36744923e-03\n",
      "  3.26654967e-03  3.81742767e-03 -4.32078587e-03  4.14358499e-03\n",
      "  4.49732586e-04  6.37035526e-04 -1.71470642e-03 -2.86701485e-03\n",
      "  4.10954189e-03  4.63616010e-03 -1.54243107e-03  4.60500922e-03\n",
      " -2.54580693e-04  3.16397217e-03  1.48711249e-03  4.55835555e-03\n",
      " -4.45452053e-03  1.73137465e-03  4.81632585e-03 -4.57772799e-03\n",
      " -2.17327126e-03 -1.75905030e-03 -2.76785716e-03 -1.91723299e-03\n",
      " -4.81358264e-03 -1.02819933e-03 -7.67725869e-04 -3.83614097e-03\n",
      " -9.09779046e-04 -2.82967609e-04 -3.95966135e-03  1.93609972e-03\n",
      " -1.48014259e-03  1.22440542e-04  1.42355182e-03  3.84764653e-03\n",
      " -3.80035024e-03 -1.68786588e-04 -2.77769077e-03  5.02424547e-03\n",
      " -1.70081342e-03  4.30700881e-03 -2.57573137e-03  1.95236687e-04\n",
      " -3.40595907e-05  4.90640383e-03 -1.64561556e-03 -1.09981350e-03\n",
      " -2.23117648e-03 -1.61494152e-03  3.22014093e-04 -2.64245528e-03\n",
      "  3.33580025e-03 -2.33134395e-03 -7.99777743e-04 -3.45459511e-03\n",
      " -1.08361919e-03  5.19469206e-04  1.15200804e-04  7.09345273e-04\n",
      " -4.42026136e-03 -9.93923284e-04 -3.43613280e-03  8.07284028e-04]\n"
     ]
    }
   ],
   "source": [
    "print(model.docvecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag = 0の文章との類似度[(5, 0.2770974636077881), (1, 0.07727166265249252), (6, 0.07696818560361862), (3, 0.028534501791000366), (2, 0.01765500009059906), (4, -0.07608857750892639)]\n",
      "tag = 1の文章との類似度[(6, 0.16894283890724182), (0, 0.07727167010307312), (5, -0.003815270960330963), (3, -0.05440355837345123), (2, -0.0580289289355278), (4, -0.1821083128452301)]\n",
      "tag = 2の文章との類似度[(4, 0.056835103780031204), (0, 0.017655007541179657), (6, -0.04060372710227966), (3, -0.05082041025161743), (1, -0.058028921484947205), (5, -0.09862123429775238)]\n",
      "tag = 3の文章との類似度[(0, 0.028534486889839172), (5, -0.022549912333488464), (4, -0.04953721910715103), (2, -0.05082041025161743), (1, -0.05440356209874153), (6, -0.08479255437850952)]\n",
      "tag = 4の文章との類似度[(2, 0.05683509260416031), (6, -0.023323191329836845), (3, -0.049537211656570435), (5, -0.0689239650964737), (0, -0.07608857750892639), (1, -0.18210828304290771)]\n",
      "tag = 5の文章との類似度[(0, 0.2770974338054657), (6, 0.12971948087215424), (1, -0.003815263509750366), (3, -0.022549912333488464), (4, -0.0689239650964737), (2, -0.09862123429775238)]\n",
      "tag = 6の文章との類似度[(1, 0.16894283890724182), (5, 0.12971948087215424), (0, 0.07696819305419922), (4, -0.023323189467191696), (2, -0.04060372710227966), (3, -0.08479255437850952)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(diary_df)):\n",
    "    print('tag = ' + str(i) +'の文章との類似度{}'.format(model.docvecs.most_similar(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
